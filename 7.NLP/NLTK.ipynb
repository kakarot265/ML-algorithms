{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab03264-63e4-4ee7-aa91-b3bf906e4fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vanshika mishra\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\vanshika mishra\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vanshika mishra\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\vanshika mishra\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\vanshika mishra\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b2be28-c288-4da7-b069-dde32f58be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644756f9-995d-4048-84b5-18c539493440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Vanshika\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea18ea0c-72ee-4797-abcf-fa2b32e51a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf85494d-e0f4-4b11-a3bd-b488eecee114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(reuters.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbcf871-3151-47ec-b5a1-ba097a39a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reuters.sents(categories='barley')#gets sentences of particular category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9664e80-ec7a-4510-ad04-5c9cb069c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the barley , rebates of between 138 and 141 . 25 European currency units ( Ecus ) per tonne were sought , for maize they were between 129 . 65 and 139 Ecus , for bread - making wheat around 145 Ecus and for feed wheat around 142 . 45 Ecus .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 sentence in fiction category\n",
    "' '.join(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97518bd9-c3df-4a7f-a8d6-6fc089080bd7",
   "metadata": {},
   "source": [
    "## Bag of Words Pipeline\n",
    "- Get the Data/Corpus\n",
    "- Tokenisation- break document--> sentences--> word\n",
    "- Stopward Removal --> discarding words that dont contribute to ML model -- like pronouns \n",
    "- Stemming/Lemmetization --> to convert diff form of same word in base word- running, runs- run \n",
    "- Building a Vocab- build a common vocab- list of unique words\n",
    "- Vectorization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260599f-301c-46d4-90b7-db8e14be6dc4",
   "metadata": {},
   "source": [
    "# Tokenization & Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03445fa7-cf28-4f55-9441-e6317b44469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers. \n",
    "I went to the market to buy some fruits.\"\"\"\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at vanshika@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2359fd-14c3-4bd2-8e0f-8b0350e599c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b751bb-33a5-43f5-9530-7995cc405919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vanshika\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f0c7ae-8fd0-4401-bb64-b6be50850082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to buy some fruits.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(document) #break document into list of sentences \n",
    "print(sents)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18253aff-e1b2-4935-9e58-ea60a4a357a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a very pleasant day.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ac63ed-7f9e-4545-a717-41a329425c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'vanshika@gmail.com']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "976e12f9-1038-4b68-b037-27dcad6547a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf7b8e78-94fc-4dfa-9972-dec90fa3825e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'vanshika',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66722529-fd9a-41f0-9b2d-338ade4dbc5a",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dbf21f1-6329-422c-8efe-f3214bcd803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vanshika\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb6cc29f-566b-4ae4-8a6d-23e4d819a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea1b0841-d3ec-40eb-a80f-52b3c97d27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yours', 'mustn', 'hers', 'but', 'ours', \"haven't\", 're', 'y', 'were', \"you've\", 'to', 'if', 'nor', 'who', 'ain', 'yourselves', 'then', 'between', 'other', 'the', 'hasn', 'is', 'out', 'be', 'off', 'has', \"wouldn't\", \"wasn't\", 'herself', 'at', 'we', 'each', 'that', 'weren', 'any', 'below', 'he', \"didn't\", 'there', \"mightn't\", 'hadn', 'should', 'now', 'both', 'whom', 't', 'against', 'she', 've', 'until', 'own', 'very', 'had', 'before', 'don', 'my', 'wouldn', \"aren't\", 'couldn', 'isn', 'too', 'did', 'you', 'shouldn', 'by', 'myself', 'a', \"that'll\", \"you're\", 'itself', 'its', 'our', 'shan', 'theirs', 'haven', 'such', 'same', 'ma', 'only', 'themselves', 'why', 'from', 'on', 'most', 'for', 'as', 'of', 'their', 'when', 'been', \"couldn't\", 'or', \"shouldn't\", 'm', 'these', \"you'll\", 'so', 'once', 'ourselves', \"you'd\", 'about', 'do', 'few', \"won't\", \"doesn't\", 'him', 'above', 'can', \"hadn't\", 'through', 'not', 'o', 'needn', 'didn', 'them', \"weren't\", 'again', 'won', 'her', 'than', 'being', 'aren', \"mustn't\", 'in', \"she's\", 'it', \"it's\", 'further', 'your', 'some', 'wasn', 'into', \"shan't\", 'they', 'down', \"should've\", 'under', \"hasn't\", 'i', 'doesn', 's', 'during', 'here', 'does', \"isn't\", 'what', 'with', 'are', 'have', 'doing', \"needn't\", 'yourself', 'those', 'and', 'me', 'am', 'which', 'an', 'all', 'this', 'having', 'just', \"don't\", 'll', 'd', 'himself', 'mightn', 'while', 'more', 'how', 'no', 'will', 'his', 'after', 'because', 'over', 'where', 'up', 'was'}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dacad956-e2c3-440d-8bd4-5f5d9fc9a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stopwords):\n",
    "    useful_words = [w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502fcbb4-adb0-44d5-9209-cdc0a1e3e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bothered', 'much']\n"
     ]
    }
   ],
   "source": [
    "text = \"i am not bothered about her very much\".split()\n",
    "useful_text = remove_stopwords(text,sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b8106-033d-4663-ba73-d87528aad027",
   "metadata": {},
   "source": [
    "## Tokenization using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79693a00-738e-434f-9844-42db617c9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a90d8250-fb99-4828-a5bb-139f5d2e3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+') #generated from regexpal\n",
    "useful_text = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "691c79fd-53da-43c7-8381-4ccbd5307fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " 'at',\n",
       " 'vanshika@gmail.com']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c170d4ab-9480-44f5-a9b0-4dc3b18dfe29",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- Process that transforms particular words(verbs,plurals)into their radical form\n",
    "- Preserve the semantics of the sentence without increasing the number of unique tokens\n",
    "- Snowball, Porter, Lancaster Stemmer\n",
    "Example - jumps, jumping, jumped, jump ==> jum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a838ced-2662-4797-877a-0e2a7a5d3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"\"\"Foxes love to make jumps.The quick brown fox was seen jumping over the \n",
    "        lovely dog from a 6ft feet high wall\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4c9aaea-bc66-4a0a-a9ac-33aa97ad0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "#Snowball Stemmer, Porter, Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "846dd265-00ee-4f35-bc8d-a206d8be109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b9e77f-f3a9-40ea-8d3f-91d4718d7d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e19be026-4bd2-445c-82af-fcdede588847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely')\n",
    "ps.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83897c0c-e14c-454e-b05b-1c1309a12248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer\n",
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4835bcc7-aed7-40b2-b0e5-0f75aa914923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2754fee5-cd87-419c-b80b-cc00596897c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faa08ada-7271-4309-b338-25070189ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Vanshika\n",
      "[nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "285fffdc-5cb8-4e90-a806-cede0353ceef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumping'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wn = WordNetLemmatizer()\n",
    "wn.lemmatize('jumping')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c418e25-fa42-4d99-8cb7-9111accc5928",
   "metadata": {},
   "source": [
    "# Building a Vocab & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "712c229d-f601-48da-9936-0084a97871f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Corpus - Contains 4 Documents, each document can have 1 or more sentences\n",
    "corpus = [\n",
    "        'Indian cricket team will win World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people.',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e978364-7476-48b7-a499-86738dd7d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bc7dcd3-2ab5-4475-82e1-0c2651d0b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "822601af-174f-49f0-b144-0a71354914f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbcbd703-6e38-4e56-99e7-c04162c8c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1b1f8e1-7428-4310-b4b6-b425a98520a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
      " 2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "len(vectorized_corpus[0])\n",
    "print(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1fefccf-12c4-4c1a-ace0-fb615906a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'win': 38, 'world': 40, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 39, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "014660f4-25eb-4642-8558-63ec981aa484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "736a6783-e52c-4dbb-8b1f-8129f9d482eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse Mapping!\n",
    "numbers = vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e17fcccd-6cc0-4a5b-8085-e5ec505fb7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "s = cv.inverse_transform(numbers)#convert nos to words\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbb769-210e-4d1a-a939-c753f578dfe9",
   "metadata": {},
   "source": [
    "## Vectorization with Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a685e63a-7fcc-45fe-9071-80aa602af6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    # Remove Stopwords\n",
    "    words = remove_stopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff89e25d-2975-4905-9a21-6c78e41e819a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'documents', 'related', 'chapters', 'vanshika@gmail.com']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9cb3fc4-01c6-4666-9e80-934b09a92555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send all the 50 documents related to chapters 1,2,3 at vanshika@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b57780e4-bb97-4f31-af46-7f80e288311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1dd87c57-e1f4-4289-977c-cffec0496e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7793b506-4ee1-4d0d-a838-a073dd1e70c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e897978c-0655-4719-932f-3388f82c0922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['capt.', 'cricket', 'cup', 'held', 'indian', 'kohli.', 'lanka.',\n",
       "        'says', 'sri', 'team', 'virat', 'win', 'world'], dtype='<U9'),\n",
       " array(['confident', 'elections', 'indian', 'lok', 'next', 'pm', 'sabha',\n",
       "        'says', 'win'], dtype='<U9'),\n",
       " array(['hearts', 'laurate', 'nobel', 'people.'], dtype='<U9'),\n",
       " array(['based', 'exciting', 'indian', 'movie', 'raazi', 'real', 'spy',\n",
       "        'story.', 'thriller', 'upon'], dtype='<U9')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c68fcf2-b5e3-4175-beb8-97d8f4ee6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test Data\n",
    "test_corpus = [\n",
    "        'Indian cricket rock !',        \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b094fea7-2165-4dcd-9e36-943feea2b1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325cd20c-1435-42cf-bc3a-a74ade62fa00",
   "metadata": {},
   "source": [
    "# More ways to Create Features\n",
    "- Unigram - every word as a feature\n",
    "- Bigrams\n",
    "- Trigrams\n",
    "- n-grams\n",
    "- TF-IDF Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae99c7a1-e058-4206-8c97-ef49890b6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1  = [\"this is good movie\"]\n",
    "sent_2 = [\"this is good movie but actor is not present\"]\n",
    "sent_3 = [\"this is not good movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88bac62c-54db-4678-826c-4434d76f7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59dd6d5a-e6da-4ef8-b933-5f2513303e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [sent_1[0],sent_2[0]]\n",
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02c5deb5-c265-4f6c-9d87-1e1227739540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 20,\n",
       " 'is': 9,\n",
       " 'good': 6,\n",
       " 'movie': 14,\n",
       " 'this is': 21,\n",
       " 'is good': 10,\n",
       " 'good movie': 7,\n",
       " 'this is good': 22,\n",
       " 'is good movie': 11,\n",
       " 'but': 3,\n",
       " 'actor': 0,\n",
       " 'not': 17,\n",
       " 'present': 19,\n",
       " 'movie but': 15,\n",
       " 'but actor': 4,\n",
       " 'actor is': 1,\n",
       " 'is not': 12,\n",
       " 'not present': 18,\n",
       " 'good movie but': 8,\n",
       " 'movie but actor': 16,\n",
       " 'but actor is': 5,\n",
       " 'actor is not': 2,\n",
       " 'is not present': 13}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abc89a-6f9b-4b8d-983d-727d1d535201",
   "metadata": {},
   "source": [
    "# Tf-idf Normalisation¶\n",
    "- Avoid features that occur very often, because they contain less information\n",
    "- Information decreases as the number of occurences increases across different type of documents\n",
    "- So we define another term - term-document-frequency which associates a weight with every term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17a7175d-0da5-4589-85b2-39ae3ec7240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1  = \"this is good movie\"\n",
    "sent_2 = \"this was good movie\"\n",
    "sent_3 = \"this is not good movie\"\n",
    "\n",
    "corpus = [sent_1,sent_2,sent_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e6a8370-a0fb-4f95-bad5-02410f280805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7346bcab-18e2-4495-b601-81a216278cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6a8dafb-301e-4d64-bc21-f1a9f4540ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "537ac8f2-3e02-4aed-9d56-38f1e0b5c28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a09895f-2f5a-4945-931b-457e8990b59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
